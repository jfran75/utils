version: "3.7"

services:
  llama.cpp:
    container_name: llama.cpp
    image: llama.cpp
    build:
      context: ../
      dockerfile: ./llama.cpp/Dockerfile
    ports:
      - 4600:8080
      # - 8081:8081
    networks:
      - sfyc
    # extra_hosts:
    #   - "localhost:127.0.0.1"
    volumes:
      - /home/jchinchillas/proyectos/rlab/utils/ai/llm_models:/models # rlaplnxml2

networks:
  sfyc:
    name: sfyc
    external: false